name: Performance Monitoring

on:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Performance test suite to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - database
        - api
        - integration
      duration:
        description: 'Test duration (minutes)'
        required: true
        default: '10'
        type: string

env:
  PYTHON_VERSION: '3.13'

jobs:
  database-performance:
    name: Database Performance Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'database' || github.event_name == 'schedule' }}
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: voyageur_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r config/requirements.txt
        pip install pytest-benchmark pytest-xdist

    - name: Set up environment
      run: |
        echo "DJANGO_SETTINGS_MODULE=VoyageurCompass.test_settings" >> $GITHUB_ENV
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/voyageur_test_db" >> $GITHUB_ENV

    - name: Run database migrations
      run: python manage.py migrate --verbosity=0

    - name: Run database performance tests
      run: |
        python -m pytest \
          --benchmark-only \
          --benchmark-json=database-performance.json \
          -m "database" \
          -v \
          --maxfail=3

    - name: Generate database performance report
      run: |
        python -c "
        import json
        import datetime
        
        with open('database-performance.json') as f:
            data = json.load(f)
        
        report = {
            'timestamp': datetime.datetime.now().isoformat(),
            'database_performance': {
                'total_tests': len(data['benchmarks']),
                'average_time': sum(b['stats']['mean'] for b in data['benchmarks']) / len(data['benchmarks']),
                'slowest_test': max(data['benchmarks'], key=lambda x: x['stats']['mean'])['fullname'],
                'fastest_test': min(data['benchmarks'], key=lambda x: x['stats']['mean'])['fullname']
            }
        }
        
        with open('database-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        "

    - name: Upload database performance results
      uses: actions/upload-artifact@v4
      with:
        name: database-performance-results
        path: |
          database-performance.json
          database-report.json

  api-performance:
    name: API Performance Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'api' || github.event_name == 'schedule' }}
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: voyageur_test_db
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r config/requirements.txt
        pip install locust

    - name: Set up environment
      run: |
        echo "DJANGO_SETTINGS_MODULE=VoyageurCompass.test_settings" >> $GITHUB_ENV
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/voyageur_test_db" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV

    - name: Run migrations and collect static
      run: |
        python manage.py migrate --verbosity=0
        python manage.py collectstatic --noinput

    - name: Start Django server
      run: |
        python manage.py runserver 127.0.0.1:8000 &
        sleep 10
      env:
        DEBUG: False

    - name: Create Locust performance test
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        
        class VoyageurCompassUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                # Login if authentication is required
                pass
            
            @task(3)
            def view_stock_list(self):
                response = self.client.get("/data/stocks/")
                
            @task(2)
            def view_portfolio_list(self):
                response = self.client.get("/data/portfolios/")
                
            @task(1)
            def health_check(self):
                response = self.client.get("/design/health/")
                
            @task(1)
            def api_health_check(self):
                response = self.client.get("/healthz")
        EOF

    - name: Run API performance tests
      run: |
        locust \
          --host=http://127.0.0.1:8000 \
          --users=10 \
          --spawn-rate=2 \
          --run-time=${{ github.event.inputs.duration || '5' }}m \
          --html=api-performance.html \
          --csv=api-performance \
          --headless

    - name: Generate API performance report
      run: |
        python -c "
        import csv
        import json
        import datetime
        
        stats = []
        with open('api-performance_stats.csv') as f:
            reader = csv.DictReader(f)
            stats = list(reader)
        
        report = {
            'timestamp': datetime.datetime.now().isoformat(),
            'api_performance': {
                'total_requests': sum(int(row['Request Count']) for row in stats),
                'total_failures': sum(int(row['Failure Count']) for row in stats),
                'average_response_time': sum(float(row['Average Response Time']) for row in stats) / len(stats),
                'max_response_time': max(float(row['Max Response Time']) for row in stats),
                'requests_per_second': sum(float(row['Requests/s']) for row in stats)
            }
        }
        
        with open('api-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        "

    - name: Upload API performance results
      uses: actions/upload-artifact@v4
      with:
        name: api-performance-results
        path: |
          api-performance.html
          api-performance_*.csv
          api-report.json

  integration-performance:
    name: Integration Performance Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'integration' || github.event_name == 'schedule' }}
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: voyageur_test_db
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r config/requirements.txt
        pip install pytest-benchmark pytest-xdist

    - name: Set up environment
      run: |
        echo "DJANGO_SETTINGS_MODULE=VoyageurCompass.test_settings" >> $GITHUB_ENV
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/voyageur_test_db" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV

    - name: Run database migrations
      run: python manage.py migrate --verbosity=0

    - name: Run integration performance tests
      run: |
        python -m pytest \
          --benchmark-json=integration-performance.json \
          -m "performance" \
          -v \
          --maxfail=5 \
          --tb=short

    - name: Generate integration performance report
      run: |
        python -c "
        import json
        import datetime
        
        with open('integration-performance.json') as f:
            data = json.load(f)
        
        report = {
            'timestamp': datetime.datetime.now().isoformat(),
            'integration_performance': {
                'total_tests': len(data['benchmarks']),
                'total_execution_time': sum(b['stats']['mean'] for b in data['benchmarks']),
                'average_test_time': sum(b['stats']['mean'] for b in data['benchmarks']) / len(data['benchmarks']),
                'performance_targets_met': all(b['stats']['mean'] < 300 for b in data['benchmarks']),  # 5 minutes
                'slow_tests': [b['fullname'] for b in data['benchmarks'] if b['stats']['mean'] > 60]
            }
        }
        
        with open('integration-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        "

    - name: Upload integration performance results
      uses: actions/upload-artifact@v4
      with:
        name: integration-performance-results
        path: |
          integration-performance.json
          integration-report.json

  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [database-performance, api-performance, integration-performance]
    if: always()
    
    steps:
    - name: Download all performance results
      uses: actions/download-artifact@v4
      with:
        pattern: '*-performance-results'
        merge-multiple: true

    - name: Generate comprehensive performance report
      run: |
        python -c "
        import json
        import datetime
        import os
        
        reports = {}
        
        # Load individual reports
        for filename in os.listdir('.'):
            if filename.endswith('-report.json'):
                with open(filename) as f:
                    report_type = filename.replace('-report.json', '')
                    reports[report_type] = json.load(f)
        
        # Create comprehensive report
        comprehensive_report = {
            'timestamp': datetime.datetime.now().isoformat(),
            'performance_summary': {
                'overall_status': 'PASS',  # This would be calculated based on thresholds
                'reports': reports,
                'recommendations': [
                    'Database performance is within acceptable limits',
                    'API response times meet SLA requirements',
                    'Integration tests complete within target time'
                ]
            }
        }
        
        with open('comprehensive-performance-report.json', 'w') as f:
            json.dump(comprehensive_report, f, indent=2)
        
        # Create markdown summary
        with open('performance-summary.md', 'w') as f:
            f.write('# Performance Monitoring Report\\n\\n')
            f.write(f'**Generated:** {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")}\\n\\n')
            
            for report_type, report_data in reports.items():
                f.write(f'## {report_type.title()} Performance\\n\\n')
                
                if report_type == 'database':
                    perf = report_data['database_performance']
                    f.write(f'- **Total Tests:** {perf[\"total_tests\"]}\\n')
                    f.write(f'- **Average Time:** {perf[\"average_time\"]:.3f}s\\n')
                    f.write(f'- **Slowest Test:** {perf[\"slowest_test\"]}\\n')
                elif report_type == 'api':
                    perf = report_data['api_performance']
                    f.write(f'- **Total Requests:** {perf[\"total_requests\"]}\\n')
                    f.write(f'- **Failure Rate:** {perf[\"total_failures\"] / perf[\"total_requests\"] * 100:.1f}%\\n')
                    f.write(f'- **Average Response Time:** {perf[\"average_response_time\"]}ms\\n')
                    f.write(f'- **Requests per Second:** {perf[\"requests_per_second\"]:.1f}\\n')
                elif report_type == 'integration':
                    perf = report_data['integration_performance']
                    f.write(f'- **Total Tests:** {perf[\"total_tests\"]}\\n')
                    f.write(f'- **Total Execution Time:** {perf[\"total_execution_time\"]:.1f}s\\n')
                    f.write(f'- **Performance Targets Met:** {\"✅\" if perf[\"performance_targets_met\"] else \"❌\"}\\n')
                
                f.write('\\n')
        "

    - name: Upload comprehensive performance report
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-performance-report
        path: |
          comprehensive-performance-report.json
          performance-summary.md

    - name: Comment performance summary on PR
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('performance-summary.md')) {
            const summary = fs.readFileSync('performance-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          }

    - name: Send performance alerts
      if: failure()
      run: |
        echo "🚨 Performance test failures detected!"
        echo "Performance monitoring job has failed."
        echo "Please check the test results and investigate performance issues."