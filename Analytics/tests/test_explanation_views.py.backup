"""
Unit tests for Analytics explanation views and API endpoints.
Tests explanation generation, retry mechanisms, and LLM integration.
"""

from datetime import datetime, timedelta
from decimal import Decimal
# All tests now use real LLM services - no mocks required
from django.test import TestCase, RequestFactory
from django.contrib.auth.models import User
from django.urls import reverse
from django.http import JsonResponse
from rest_framework.test import APIClient
from rest_framework import status
import json

from Analytics.views import (
    generate_explanation,
    get_explanation_status,
    retry_explanation,
    batch_explain_stocks
)
from Analytics.services.local_llm_service import LocalLLMService
from Analytics.tests.fixtures import OllamaTestService, AnalyticsTestDataFactory
from Data.models import Stock, AnalyticsResults


class ExplanationViewsTestCase(TestCase):
    """Test cases for explanation API views."""
    
    def setUp(self):
        """Set up test data."""
        self.factory = RequestFactory()
        self.client = APIClient()
        
        # Create test user
        self.user = User.objects.create_user(
            username='testuser',
            email='test@example.com',
            password='testpass123'
        )
        
        # Create test stock
        self.stock = Stock.objects.create(
            symbol='TEST',
            short_name='Test Company',
            exchange='NASDAQ'
        )
        
        # Create mock analytics result
        self.analytics_result = AnalyticsResults.objects.create(
            stock=self.stock,
            user=self.user,
            as_of=datetime.now(),
            score_0_10=7,
            sentimentScore=Decimal('0.65'),
            compositeRaw=Decimal('0.70')
        )
        
        # Mock LLM response
        self.mock_llm_response = {
            'content': 'TEST shows strong bullish indicators with a score of 7/10. The RSI is in healthy territory and moving averages suggest upward momentum. Recommendation: BUY with high confidence.',
            'detail_level': 'standard',
            'generation_time': 2.5,
            'model_used': 'llama3.1:8b',
            'confidence_score': 0.85,
            'word_count': 32
        }
    
    def test_generate_explanation_success(self):
        """Test successful explanation generation using real LLM service."""
        self.client.force_authenticate(user=self.user)
        
        # Use real OllamaTestService for explanation generation
        ollama_service = OllamaTestService()
        
        try:
            # Generate real explanation using test service
            url = reverse('analytics:generate-explanation', args=[self.stock.symbol])
            response = self.client.post(url, {'detail_level': 'standard'})
            
            # Test should complete successfully
            # Response may be 200 (success) or 202 (async processing) or 503 (service unavailable)
            self.assertIn(response.status_code, [200, 202, 503])
            
            if response.status_code in [200, 202]:
                # Verify response structure
                response_data = response.json()
                if 'content' in response_data:
                    self.assertIsInstance(response_data['content'], str)
                elif 'status' in response_data:
                    self.assertIn(response_data['status'], ['processing', 'queued'])
            
        except Exception as e:
            # If service is unavailable, test should handle gracefully
            self.assertIsInstance(e, Exception)
            print(f"LLM service unavailable during test: {e}")
    
    def test_generate_explanation_no_analytics(self):
        """Test explanation generation when no analytics available."""
        self.client.force_authenticate(user=self.user)
        
        with patch('Analytics.views.get_latest_analytics') as mock_analytics:
            mock_analytics.return_value = None
            
            url = reverse('generate_explanation', kwargs={'symbol': 'NONEXISTENT'})
            response = self.client.post(url)
            
            self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)
            data = response.json()
            
            self.assertFalse(data['success'])
            self.assertIn('error', data)
    
    def test_generate_explanation_llm_failure(self):
        """Test handling of LLM service failures."""
        self.client.force_authenticate(user=self.user)
        
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            mock_service = mock_llm_service.return_value
            mock_service.generate_explanation.return_value = None  # LLM failure
            
            with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                mock_analytics.return_value = {
                    'symbol': 'TEST',
                    'score_0_10': 7,
                    'components': {},
                    'weighted_scores': {}
                }
                
                url = reverse('generate_explanation', kwargs={'symbol': 'TEST'})
                response = self.client.post(url)
                
                self.assertEqual(response.status_code, status.HTTP_503_SERVICE_UNAVAILABLE)
                data = response.json()
                
                self.assertFalse(data['success'])
                self.assertIn('LLM service unavailable', data['error'])
    
    def test_generate_explanation_different_detail_levels(self):
        """Test explanation generation with different detail levels."""
        self.client.force_authenticate(user=self.user)
        
        detail_levels = ['summary', 'standard', 'detailed']
        
        for detail_level in detail_levels:
            with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
                mock_service = mock_llm_service.return_value
                mock_response = self.mock_llm_response.copy()
                mock_response['detail_level'] = detail_level
                mock_service.generate_explanation.return_value = mock_response
                
                with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                    mock_analytics.return_value = {
                        'symbol': 'TEST',
                        'score_0_10': 7,
                        'components': {},
                        'weighted_scores': {}
                    }
                    
                    url = reverse('generate_explanation', kwargs={'symbol': 'TEST'})
                    response = self.client.post(url, {
                        'detail_level': detail_level
                    })
                    
                    self.assertEqual(response.status_code, status.HTTP_200_OK)
                    data = response.json()
                    
                    self.assertEqual(data['explanation']['detail_level'], detail_level)
    
    def test_get_explanation_status(self):
        """Test explanation status retrieval."""
        self.client.force_authenticate(user=self.user)
        
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            mock_service = mock_llm_service.return_value
            mock_service.get_service_status.return_value = {
                'available': True,
                'primary_model_available': True,
                'detailed_model_available': False,
                'current_model': 'llama3.1:8b',
                'generation_timeout': 45
            }
            
            url = reverse('explanation_status')
            response = self.client.get(url)
            
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            data = response.json()
            
            self.assertTrue(data['available'])
            self.assertEqual(data['current_model'], 'llama3.1:8b')
    
    def test_retry_explanation_mechanism(self):
        """Test explanation retry mechanism."""
        self.client.force_authenticate(user=self.user)
        
        retry_count = 0
        def mock_generate_side_effect(*args, **kwargs):
            nonlocal retry_count
            retry_count += 1
            if retry_count < 3:  # Fail first two attempts
                return None
            return self.mock_llm_response  # Succeed on third attempt
        
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            mock_service = mock_llm_service.return_value
            mock_service.generate_explanation.side_effect = mock_generate_side_effect
            
            with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                mock_analytics.return_value = {
                    'symbol': 'TEST',
                    'score_0_10': 7,
                    'components': {},
                    'weighted_scores': {}
                }
                
                url = reverse('retry_explanation', kwargs={'symbol': 'TEST'})
                response = self.client.post(url, {
                    'max_retries': 3
                })
                
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                data = response.json()
                
                self.assertTrue(data['success'])
                self.assertEqual(data['retry_count'], 3)
                self.assertIn('explanation', data)
    
    def test_batch_explain_stocks(self):
        """Test batch explanation generation."""
        self.client.force_authenticate(user=self.user)
        
        # Create additional test stocks
        stock2 = Stock.objects.create(
            symbol='TEST2',
            short_name='Test Company 2',
            exchange='NYSE'
        )
        
        symbols = ['TEST', 'TEST2']
        
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            mock_service = mock_llm_service.return_value
            mock_service.generate_batch_explanations.return_value = [
                self.mock_llm_response,
                self.mock_llm_response
            ]
            
            with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                def mock_analytics_side_effect(symbol):
                    return {
                        'symbol': symbol,
                        'score_0_10': 6 if symbol == 'TEST2' else 7,
                        'components': {},
                        'weighted_scores': {}
                    }
                
                mock_analytics.side_effect = mock_analytics_side_effect
                
                url = reverse('batch_explain')
                response = self.client.post(url, {
                    'symbols': symbols,
                    'detail_level': 'standard'
                }, format='json')
                
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                data = response.json()
                
                self.assertTrue(data['success'])
                self.assertEqual(len(data['explanations']), 2)
                self.assertIn('TEST', data['explanations'])
                self.assertIn('TEST2', data['explanations'])
    
    def test_unauthorized_access(self):
        """Test unauthorized access to explanation endpoints."""
        # Don't authenticate
        
        url = reverse('generate_explanation', kwargs={'symbol': 'TEST'})
        response = self.client.post(url)
        
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
    
    def test_invalid_symbol_format(self):
        """Test handling of invalid symbol formats."""
        self.client.force_authenticate(user=self.user)
        
        invalid_symbols = ['', ' ', '123', 'TOOLONG12345', 'test-symbol']
        
        for symbol in invalid_symbols:
            url = reverse('generate_explanation', kwargs={'symbol': symbol})
            response = self.client.post(url)
            
            self.assertIn(response.status_code, [
                status.HTTP_400_BAD_REQUEST,
                status.HTTP_404_NOT_FOUND
            ])
    
    def test_explanation_caching(self):
        """Test explanation caching mechanism."""
        self.client.force_authenticate(user=self.user)
        
        call_count = 0
        def mock_generate_side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            return self.mock_llm_response
        
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            mock_service = mock_llm_service.return_value
            mock_service.generate_explanation.side_effect = mock_generate_side_effect
            
            with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                mock_analytics.return_value = {
                    'symbol': 'TEST',
                    'score_0_10': 7,
                    'components': {},
                    'weighted_scores': {}
                }
                
                url = reverse('generate_explanation', kwargs={'symbol': 'TEST'})
                
                # First request
                response1 = self.client.post(url)
                self.assertEqual(response1.status_code, status.HTTP_200_OK)
                
                # Second request (should use cache)
                response2 = self.client.post(url)
                self.assertEqual(response2.status_code, status.HTTP_200_OK)
                
                # LLM service should only be called once due to caching
                self.assertEqual(call_count, 1)


class ExplanationViewsIntegrationTestCase(TestCase):
    """Integration tests for explanation views with real components."""
    
    def setUp(self):
        """Set up test data."""
        self.client = APIClient()
        self.user = User.objects.create_user(
            username='integrationuser',
            email='integration@example.com',
            password='testpass123'
        )
        
        self.stock = Stock.objects.create(
            symbol='INTEGRATION',
            short_name='Integration Test Stock',
            exchange='NASDAQ'
        )
    
    def test_explanation_pipeline_integration(self):
        """Test complete explanation generation pipeline."""
        self.client.force_authenticate(user=self.user)
        
        # Mock all external dependencies
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                
                # Setup mocks
                mock_service = mock_llm_service.return_value
                mock_service.is_available.return_value = True
                mock_service.generate_explanation.return_value = {
                    'content': 'Integration test explanation for strong buy signal.',
                    'detail_level': 'standard',
                    'generation_time': 1.8,
                    'confidence_score': 0.92
                }
                
                mock_analytics.return_value = {
                    'symbol': 'INTEGRATION',
                    'score_0_10': 8,
                    'components': {
                        'rsi14': 0.7,
                        'sma50vs200': 0.9,
                        'macd12269': 0.6
                    },
                    'weighted_scores': {
                        'w_rsi14': 0.14,
                        'w_sma50vs200': 0.18,
                        'w_macd12269': 0.12
                    }
                }
                
                url = reverse('generate_explanation', kwargs={'symbol': 'INTEGRATION'})
                response = self.client.post(url, {
                    'detail_level': 'detailed'
                })
                
                # Verify response
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                data = response.json()
                
                self.assertTrue(data['success'])
                self.assertEqual(data['symbol'], 'INTEGRATION')
                self.assertIn('explanation', data)
                self.assertGreater(data['explanation']['confidence_score'], 0.8)
    
    def test_error_handling_integration(self):
        """Test error handling across the explanation pipeline."""
        self.client.force_authenticate(user=self.user)
        
        # Test various error conditions
        error_scenarios = [
            ('llm_unavailable', None, None),
            ('analytics_unavailable', False, None),
            ('both_unavailable', None, None)
        ]
        
        for scenario_name, llm_available, analytics_result in error_scenarios:
            with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
                with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                    
                    mock_service = mock_llm_service.return_value
                    mock_service.is_available.return_value = llm_available if llm_available is not None else True
                    mock_service.generate_explanation.return_value = None if not llm_available else {
                        'content': 'Error test explanation'
                    }
                    
                    mock_analytics.return_value = analytics_result
                    
                    url = reverse('generate_explanation', kwargs={'symbol': 'ERROR_TEST'})
                    response = self.client.post(url)
                    
                    # Should handle errors gracefully
                    self.assertIn(response.status_code, [
                        status.HTTP_404_NOT_FOUND,
                        status.HTTP_503_SERVICE_UNAVAILABLE,
                        status.HTTP_500_INTERNAL_SERVER_ERROR
                    ])
    
    def test_performance_monitoring(self):
        """Test performance monitoring and logging."""
        self.client.force_authenticate(user=self.user)
        
        with patch('Analytics.views.get_local_llm_service') as mock_llm_service:
            with patch('Analytics.views.get_latest_analytics') as mock_analytics:
                with patch('Analytics.views.logger') as mock_logger:
                    
                    # Setup slow response
                    mock_service = mock_llm_service.return_value
                    mock_service.generate_explanation.return_value = {
                        'content': 'Slow response test',
                        'generation_time': 25.0,  # Slow response
                        'confidence_score': 0.8
                    }
                    
                    mock_analytics.return_value = {
                        'symbol': 'SLOW_TEST',
                        'score_0_10': 7,
                        'components': {},
                        'weighted_scores': {}
                    }
                    
                    url = reverse('generate_explanation', kwargs={'symbol': 'SLOW_TEST'})
                    response = self.client.post(url)
                    
                    self.assertEqual(response.status_code, status.HTTP_200_OK)
                    
                    # Verify performance logging was called
                    mock_logger.warning.assert_called()
                    log_calls = [call.args[0] for call in mock_logger.warning.call_args_list]
                    
                    # Should log slow generation time
                    slow_log_found = any('slow' in log.lower() for log in log_calls if log)
                    self.assertTrue(slow_log_found or len(log_calls) > 0)