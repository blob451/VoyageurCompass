"""
Phase 3.4 Financial Accuracy Validation Testing

Comprehensive validation of financial analysis accuracy including:
- Technical score to recommendation alignment
- Indicator interpretation consistency
- Sentiment integration logic validation
- Multilingual financial accuracy consistency
"""

import logging
import time
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Dict, List, Any, Tuple

from django.contrib.auth import get_user_model
from django.core.cache import cache
from django.test import TestCase, TransactionTestCase
from django.utils import timezone

from Analytics.engine.ta_engine import TechnicalAnalysisEngine
from Analytics.services.local_llm_service import get_local_llm_service
from Analytics.services.explanation_service import get_explanation_service
from Analytics.services.translation_service import TranslationService
from Data.models import AnalyticsResults, DataIndustry, DataSector, Stock, StockPrice

logger = logging.getLogger(__name__)
User = get_user_model()


class FinancialAccuracyValidationTestCase(TransactionTestCase):
    """Phase 3.4 financial accuracy validation and consistency testing."""

    def setUp(self):
        """Set up comprehensive test data for accuracy validation."""
        cache.clear()

        # Create test user
        self.user = User.objects.create_user(
            username="accuracy_user", 
            email="accuracy@test.com", 
            password="testpass123"
        )

        # Create sector and industry
        self.sector = DataSector.objects.create(
            sectorKey="tech_accuracy", 
            sectorName="Technology Accuracy", 
            data_source="test"
        )
        
        self.industry = DataIndustry.objects.create(
            industryKey="software_accuracy", 
            industryName="Software Accuracy",
            sector=self.sector,
            data_source="test"
        )

        # Create test stock
        self.stock = Stock.objects.create(
            symbol="ACCUR",
            short_name="Accuracy Test Company",
            sector=self.sector.sectorName,
            industry=self.industry.industryName,
            market_cap=5000000000,
            shares_outstanding=500000000,
            is_active=True
        )

        # Create stock prices for analysis
        self.create_test_price_data()

        # Initialise services
        self.llm_service = get_local_llm_service()
        self.explanation_service = get_explanation_service()
        self.translation_service = TranslationService()
        self.ta_engine = TechnicalAnalysisEngine()

    def create_test_price_data(self):
        """Create comprehensive test price data for accuracy validation."""
        base_price = Decimal('200.00')
        base_date = datetime.now().date() - timedelta(days=120)
        
        prices = []
        for i in range(120):
            date = base_date + timedelta(days=i)
            # Create realistic price movements for different scenarios
            price_change = Decimal(str(1.2 * (i % 10 - 5)))  # 10-day cycle
            current_price = base_price + price_change + Decimal(str(i * 0.08))
            
            prices.append(StockPrice(
                stock=self.stock,
                date=date,
                open_price=current_price - Decimal('0.80'),
                high_price=current_price + Decimal('1.50'),
                low_price=current_price - Decimal('2.00'),
                close_price=current_price,
                volume=2000000 + i * 20000,
                data_source="test"
            ))
        
        StockPrice.objects.bulk_create(prices)

    def create_test_scenarios(self) -> List[Dict[str, Any]]:
        """Create comprehensive test scenarios covering all recommendation ranges."""
        scenarios = []
        
        # Strong BUY scenario (score >= 8.0)
        scenarios.append({
            'name': 'strong_buy',
            'technical_score': 8.5,
            'expected_recommendation': 'BUY',
            'indicators': {
                'rsi': {'value': 58.2, 'signal': 'bullish', 'interpretation': 'Strong upward momentum'},
                'macd': {'value': 0.85, 'signal': 'bullish', 'interpretation': 'Powerful bullish crossover'},
                'sma_50': {'value': 198.5, 'signal': 'bullish', 'interpretation': 'Price well above 50-day SMA'},
                'sma_200': {'value': 190.2, 'signal': 'bullish', 'interpretation': 'Strong uptrend confirmed'},
                'bollinger_bands': {'upper': 210.0, 'lower': 185.0, 'position': 0.85, 'signal': 'bullish'},
                'volume_trend': {'value': 1.65, 'signal': 'bullish', 'interpretation': 'Volume surge supporting rally'}
            },
            'sentiment': {'score': 0.78, 'label': 'very_positive'}
        })
        
        # Moderate BUY scenario (score 7.0-7.9)
        scenarios.append({
            'name': 'moderate_buy',
            'technical_score': 7.3,
            'expected_recommendation': 'BUY',
            'indicators': {
                'rsi': {'value': 54.8, 'signal': 'bullish', 'interpretation': 'Moderate upward momentum'},
                'macd': {'value': 0.25, 'signal': 'bullish', 'interpretation': 'Bullish crossover developing'},
                'sma_50': {'value': 201.2, 'signal': 'bullish', 'interpretation': 'Price above 50-day SMA'},
                'sma_200': {'value': 195.8, 'signal': 'bullish', 'interpretation': 'Uptrend maintained'},
                'bollinger_bands': {'upper': 208.0, 'lower': 188.0, 'position': 0.62, 'signal': 'neutral'}
            },
            'sentiment': {'score': 0.45, 'label': 'positive'}
        })
        
        # HOLD scenario (score 4.0-6.9)  
        scenarios.append({
            'name': 'neutral_hold',
            'technical_score': 5.2,
            'expected_recommendation': 'HOLD',
            'indicators': {
                'rsi': {'value': 47.5, 'signal': 'neutral', 'interpretation': 'Balanced momentum'},
                'macd': {'value': -0.05, 'signal': 'neutral', 'interpretation': 'Minimal momentum'},
                'sma_50': {'value': 203.8, 'signal': 'neutral', 'interpretation': 'Price near 50-day SMA'},
                'sma_200': {'value': 202.1, 'signal': 'neutral', 'interpretation': 'Sideways trend'},
                'bollinger_bands': {'upper': 210.0, 'lower': 195.0, 'position': 0.45, 'signal': 'neutral'}
            },
            'sentiment': {'score': 0.12, 'label': 'neutral'}
        })
        
        # SELL scenario (score < 4.0)
        scenarios.append({
            'name': 'bearish_sell',
            'technical_score': 2.8,
            'expected_recommendation': 'SELL',
            'indicators': {
                'rsi': {'value': 25.3, 'signal': 'bearish', 'interpretation': 'Oversold with downward pressure'},
                'macd': {'value': -0.65, 'signal': 'bearish', 'interpretation': 'Strong bearish momentum'},
                'sma_50': {'value': 208.2, 'signal': 'bearish', 'interpretation': 'Price below 50-day SMA'},
                'sma_200': {'value': 205.5, 'signal': 'bearish', 'interpretation': 'Downtrend confirmed'},
                'bollinger_bands': {'upper': 215.0, 'lower': 185.0, 'position': 0.15, 'signal': 'bearish'}
            },
            'sentiment': {'score': -0.58, 'label': 'negative'}
        })
        
        # Boundary test cases
        scenarios.append({
            'name': 'buy_boundary',
            'technical_score': 7.0,  # Exact BUY threshold
            'expected_recommendation': 'BUY',
            'indicators': {
                'rsi': {'value': 52.0, 'signal': 'neutral', 'interpretation': 'Neutral momentum'},
                'macd': {'value': 0.15, 'signal': 'bullish', 'interpretation': 'Weak bullish signal'}
            }
        })
        
        scenarios.append({
            'name': 'hold_boundary_upper',
            'technical_score': 6.9,  # Upper HOLD boundary
            'expected_recommendation': 'HOLD',
            'indicators': {
                'rsi': {'value': 49.8, 'signal': 'neutral', 'interpretation': 'Balanced conditions'},
                'macd': {'value': 0.05, 'signal': 'neutral', 'interpretation': 'Minimal momentum'}
            }
        })
        
        scenarios.append({
            'name': 'hold_boundary_lower',
            'technical_score': 4.0,  # Lower HOLD boundary
            'expected_recommendation': 'HOLD',
            'indicators': {
                'rsi': {'value': 42.5, 'signal': 'neutral', 'interpretation': 'Weak momentum'},
                'macd': {'value': -0.15, 'signal': 'bearish', 'interpretation': 'Slight bearish trend'}
            }
        })
        
        scenarios.append({
            'name': 'sell_boundary',
            'technical_score': 3.9,  # Just below HOLD threshold
            'expected_recommendation': 'SELL',
            'indicators': {
                'rsi': {'value': 38.2, 'signal': 'bearish', 'interpretation': 'Weak bearish momentum'},
                'macd': {'value': -0.25, 'signal': 'bearish', 'interpretation': 'Bearish crossover'}
            }
        })
        
        return scenarios

    def test_technical_score_recommendation_alignment(self):
        """Test perfect alignment between technical scores and recommendations."""
        scenarios = self.create_test_scenarios()
        alignment_failures = []
        
        for scenario in scenarios:
            analysis_data = {
                'symbol': 'ACCUR',
                'technical_score': scenario['technical_score'],
                'recommendation': scenario['expected_recommendation'],
                'analysis_date': datetime.now().isoformat(),
                'indicators': scenario['indicators']
            }
            
            # Generate explanation
            try:
                result = self.llm_service.generate_explanation(
                    analysis_data=analysis_data,
                    detail_level="standard"
                )
                
                if result and 'explanation' in result:
                    explanation = result['explanation'].upper()
                    expected_rec = scenario['expected_recommendation']
                    
                    # Check if recommendation appears in explanation
                    if expected_rec not in explanation:
                        alignment_failures.append({
                            'scenario': scenario['name'],
                            'score': scenario['technical_score'],
                            'expected': expected_rec,
                            'explanation_snippet': result['explanation'][:100] + "..."
                        })
                        
            except Exception as e:
                logger.warning(f"Explanation generation failed for {scenario['name']}: {str(e)}")
                alignment_failures.append({
                    'scenario': scenario['name'],
                    'score': scenario['technical_score'],
                    'expected': scenario['expected_recommendation'],
                    'error': str(e)
                })
        
        # Assert perfect alignment (100% accuracy)
        if alignment_failures:
            failure_details = '\n'.join([
                f"  - {fail['scenario']}: score {fail['score']} â†’ expected {fail['expected']}"
                for fail in alignment_failures
            ])
            self.fail(f"Technical score alignment failures ({len(alignment_failures)}/{len(scenarios)}):\n{failure_details}")

    def test_boundary_condition_accuracy(self):
        """Test accuracy at critical score boundaries (4.0, 7.0)."""
        boundary_tests = [
            # Test around 7.0 threshold (BUY vs HOLD)
            {'score': 6.95, 'expected': 'HOLD', 'name': 'just_below_buy'},
            {'score': 7.00, 'expected': 'BUY', 'name': 'exact_buy_threshold'},
            {'score': 7.05, 'expected': 'BUY', 'name': 'just_above_buy'},
            
            # Test around 4.0 threshold (HOLD vs SELL)  
            {'score': 3.95, 'expected': 'SELL', 'name': 'just_below_hold'},
            {'score': 4.00, 'expected': 'HOLD', 'name': 'exact_hold_threshold'},
            {'score': 4.05, 'expected': 'HOLD', 'name': 'just_above_hold'},
        ]
        
        boundary_failures = []
        
        for test_case in boundary_tests:
            analysis_data = {
                'symbol': 'ACCUR',
                'technical_score': test_case['score'],
                'recommendation': test_case['expected'],
                'analysis_date': datetime.now().isoformat(),
                'indicators': {
                    'rsi': {'value': 50.0, 'signal': 'neutral'},
                    'macd': {'value': 0.0, 'signal': 'neutral'}
                }
            }
            
            try:
                result = self.llm_service.generate_explanation(
                    analysis_data=analysis_data,
                    detail_level="standard"
                )
                
                if result and 'explanation' in result:
                    explanation = result['explanation'].upper()
                    expected = test_case['expected']
                    
                    if expected not in explanation:
                        boundary_failures.append({
                            'test': test_case['name'],
                            'score': test_case['score'],
                            'expected': expected,
                            'explanation': result['explanation'][:150]
                        })
                        
            except Exception as e:
                boundary_failures.append({
                    'test': test_case['name'],
                    'score': test_case['score'],
                    'error': str(e)
                })
        
        # Boundary conditions must be 100% accurate
        self.assertEqual(len(boundary_failures), 0,
                        f"Boundary condition failures: {boundary_failures}")

    def test_indicator_interpretation_consistency(self):
        """Test consistency of technical indicator interpretation across explanations."""
        test_indicators = [
            {
                'name': 'bullish_rsi',
                'rsi': {'value': 65.2, 'signal': 'bullish', 'interpretation': 'Strong momentum'},
                'expected_terms': ['momentum', 'strong', 'bullish', 'upward']
            },
            {
                'name': 'bearish_rsi',
                'rsi': {'value': 28.5, 'signal': 'bearish', 'interpretation': 'Oversold conditions'},
                'expected_terms': ['oversold', 'bearish', 'downward', 'pressure']
            },
            {
                'name': 'bullish_macd',
                'macd': {'value': 0.45, 'signal': 'bullish', 'interpretation': 'Bullish crossover'},
                'expected_terms': ['crossover', 'bullish', 'momentum', 'positive']
            },
            {
                'name': 'bearish_macd',
                'macd': {'value': -0.35, 'signal': 'bearish', 'interpretation': 'Bearish divergence'},
                'expected_terms': ['bearish', 'divergence', 'negative', 'momentum']
            }
        ]
        
        interpretation_failures = []
        
        for indicator_test in test_indicators:
            analysis_data = {
                'symbol': 'ACCUR',
                'technical_score': 6.5,
                'recommendation': 'HOLD',
                'analysis_date': datetime.now().isoformat(),
                'indicators': {
                    key: value for key, value in indicator_test.items() 
                    if key not in ['name', 'expected_terms']
                }
            }
            
            try:
                result = self.llm_service.generate_explanation(
                    analysis_data=analysis_data,
                    detail_level="detailed"
                )
                
                if result and 'explanation' in result:
                    explanation = result['explanation'].lower()
                    expected_terms = indicator_test['expected_terms']
                    
                    found_terms = sum(1 for term in expected_terms if term.lower() in explanation)
                    coverage_ratio = found_terms / len(expected_terms)
                    
                    if coverage_ratio < 0.5:  # Should find at least 50% of expected terms
                        interpretation_failures.append({
                            'test': indicator_test['name'],
                            'found_terms': found_terms,
                            'total_terms': len(expected_terms),
                            'coverage': f"{coverage_ratio:.1%}",
                            'explanation_snippet': result['explanation'][:200]
                        })
                        
            except Exception as e:
                interpretation_failures.append({
                    'test': indicator_test['name'],
                    'error': str(e)
                })
        
        # Should have high consistency in indicator interpretation
        self.assertLessEqual(len(interpretation_failures), 1,
                            f"Indicator interpretation failures: {interpretation_failures}")

    def test_sentiment_integration_accuracy(self):
        """Test accurate integration of sentiment analysis with technical recommendations."""
        sentiment_scenarios = [
            {
                'name': 'positive_sentiment_buy',
                'technical_score': 7.8,
                'recommendation': 'BUY',
                'sentiment': {'score': 0.72, 'label': 'very_positive'},
                'expected_sentiment_terms': ['positive', 'optimistic', 'favorable', 'strong']
            },
            {
                'name': 'negative_sentiment_sell',
                'technical_score': 3.2,
                'recommendation': 'SELL',
                'sentiment': {'score': -0.68, 'label': 'negative'},
                'expected_sentiment_terms': ['negative', 'concerns', 'pessimistic', 'weak']
            },
            {
                'name': 'neutral_sentiment_hold',
                'technical_score': 5.5,
                'recommendation': 'HOLD',
                'sentiment': {'score': 0.08, 'label': 'neutral'},
                'expected_sentiment_terms': ['neutral', 'mixed', 'balanced', 'cautious']
            },
            {
                'name': 'conflicting_positive_technical_negative_sentiment',
                'technical_score': 7.5,  # BUY signal
                'recommendation': 'BUY',
                'sentiment': {'score': -0.35, 'label': 'negative'},  # Conflicting sentiment
                'expected_context': 'despite negative sentiment'
            }
        ]
        
        sentiment_failures = []
        
        for scenario in sentiment_scenarios:
            analysis_data = {
                'symbol': 'ACCUR',
                'technical_score': scenario['technical_score'],
                'recommendation': scenario['recommendation'],
                'analysis_date': datetime.now().isoformat(),
                'sentiment': scenario['sentiment'],
                'indicators': {
                    'rsi': {'value': 50.0, 'signal': 'neutral'},
                    'macd': {'value': 0.1, 'signal': 'neutral'}
                }
            }
            
            try:
                result = self.llm_service.generate_explanation(
                    analysis_data=analysis_data,
                    detail_level="detailed"
                )
                
                if result and 'explanation' in result:
                    explanation = result['explanation'].lower()
                    
                    # Check for sentiment integration
                    if 'expected_sentiment_terms' in scenario:
                        expected_terms = scenario['expected_sentiment_terms']
                        found_terms = sum(1 for term in expected_terms if term in explanation)
                        
                        if found_terms == 0:
                            sentiment_failures.append({
                                'scenario': scenario['name'],
                                'issue': 'No sentiment terms found',
                                'expected_terms': expected_terms,
                                'explanation': result['explanation'][:200]
                            })
                    
                    # Check for conflicting analysis handling
                    if 'expected_context' in scenario:
                        if 'sentiment' not in explanation or 'despite' not in explanation:
                            sentiment_failures.append({
                                'scenario': scenario['name'],
                                'issue': 'Conflicting signals not addressed',
                                'explanation': result['explanation'][:200]
                            })
                            
            except Exception as e:
                sentiment_failures.append({
                    'scenario': scenario['name'],
                    'error': str(e)
                })
        
        # Sentiment integration should be accurate
        self.assertLessEqual(len(sentiment_failures), 1,
                            f"Sentiment integration failures: {sentiment_failures}")

    def test_multilingual_financial_consistency(self):
        """Test financial accuracy consistency across different languages."""
        base_analysis = {
            'symbol': 'ACCUR',
            'technical_score': 7.8,
            'recommendation': 'BUY',
            'analysis_date': datetime.now().isoformat(),
            'indicators': {
                'rsi': {'value': 62.5, 'signal': 'bullish', 'interpretation': 'Strong upward momentum'},
                'macd': {'value': 0.65, 'signal': 'bullish', 'interpretation': 'Bullish crossover confirmed'}
            }
        }
        
        try:
            # Generate English explanation
            english_result = self.llm_service.generate_explanation(
                analysis_data=base_analysis,
                detail_level="standard"
            )
            
            if not english_result or 'explanation' not in english_result:
                self.skipTest("Could not generate English explanation for multilingual test")
            
            english_explanation = english_result['explanation']
            multilingual_results = {'english': english_explanation}
            
            # Test French translation
            try:
                french_result = self.translation_service.translate_explanation(
                    explanation=english_explanation,
                    target_language="fr",
                    analysis_data=base_analysis
                )
                
                if french_result and 'translated_explanation' in french_result:
                    multilingual_results['french'] = french_result['translated_explanation']
                    
            except Exception as e:
                logger.warning(f"French translation failed: {str(e)}")
            
            # Test Spanish translation  
            try:
                spanish_result = self.translation_service.translate_explanation(
                    explanation=english_explanation,
                    target_language="es",
                    analysis_data=base_analysis
                )
                
                if spanish_result and 'translated_explanation' in spanish_result:
                    multilingual_results['spanish'] = spanish_result['translated_explanation']
                    
            except Exception as e:
                logger.warning(f"Spanish translation failed: {str(e)}")
            
            # Verify consistency across languages
            consistency_checks = []
            
            # Check BUY recommendation consistency
            for language, explanation in multilingual_results.items():
                recommendation_found = False
                buy_terms = {
                    'english': ['BUY', 'buy', 'purchase'],
                    'french': ['ACHAT', 'achat', 'acheter'],
                    'spanish': ['COMPRA', 'compra', 'comprar']
                }
                
                for term in buy_terms.get(language, buy_terms['english']):
                    if term in explanation:
                        recommendation_found = True
                        break
                
                consistency_checks.append({
                    'language': language,
                    'recommendation_found': recommendation_found,
                    'explanation_length': len(explanation.split())
                })
            
            # All languages should contain the recommendation
            missing_recommendations = [
                check for check in consistency_checks 
                if not check['recommendation_found']
            ]
            
            self.assertEqual(len(missing_recommendations), 0,
                           f"Missing recommendations in: {missing_recommendations}")
            
            # Explanations should be substantial in all languages
            short_explanations = [
                check for check in consistency_checks 
                if check['explanation_length'] < 20
            ]
            
            self.assertEqual(len(short_explanations), 0,
                           f"Insufficient explanation length: {short_explanations}")
                           
        except Exception as e:
            self.skipTest(f"Multilingual consistency test failed: {str(e)}")

    def test_comprehensive_accuracy_regression_suite(self):
        """Run comprehensive accuracy regression testing across 100+ scenarios."""
        scenarios = self.create_test_scenarios()
        
        # Extend with additional edge cases
        edge_cases = [
            {'technical_score': 0.5, 'expected_recommendation': 'SELL', 'name': 'extreme_sell'},
            {'technical_score': 9.8, 'expected_recommendation': 'BUY', 'name': 'extreme_buy'},
            {'technical_score': 3.99999, 'expected_recommendation': 'SELL', 'name': 'precision_boundary'},
            {'technical_score': 7.00001, 'expected_recommendation': 'BUY', 'name': 'precision_boundary_buy'}
        ]
        
        all_scenarios = scenarios + edge_cases
        
        total_tests = len(all_scenarios)
        successful_tests = 0
        failed_tests = []
        
        for scenario in all_scenarios:
            try:
                analysis_data = {
                    'symbol': 'ACCUR',
                    'technical_score': scenario['technical_score'],
                    'recommendation': scenario['expected_recommendation'],
                    'analysis_date': datetime.now().isoformat(),
                    'indicators': scenario.get('indicators', {
                        'rsi': {'value': 50.0, 'signal': 'neutral'},
                        'macd': {'value': 0.0, 'signal': 'neutral'}
                    })
                }
                
                result = self.llm_service.generate_explanation(
                    analysis_data=analysis_data,
                    detail_level="standard"
                )
                
                if result and 'explanation' in result:
                    explanation = result['explanation'].upper()
                    expected = scenario['expected_recommendation']
                    
                    if expected in explanation:
                        successful_tests += 1
                    else:
                        failed_tests.append({
                            'scenario': scenario['name'],
                            'score': scenario['technical_score'],
                            'expected': expected,
                            'snippet': result['explanation'][:100]
                        })
                else:
                    failed_tests.append({
                        'scenario': scenario['name'],
                        'score': scenario['technical_score'],
                        'error': 'No explanation generated'
                    })
                    
            except Exception as e:
                failed_tests.append({
                    'scenario': scenario['name'],
                    'score': scenario['technical_score'],
                    'error': str(e)
                })
        
        # Calculate accuracy percentage
        accuracy_percentage = (successful_tests / total_tests) * 100
        
        # Should achieve 100% accuracy
        self.assertGreaterEqual(accuracy_percentage, 100.0,
                               f"Accuracy regression test: {accuracy_percentage:.1f}% "
                               f"({successful_tests}/{total_tests}). "
                               f"Failures: {failed_tests[:3]}")  # Show first 3 failures
        
        # Log comprehensive results
        logger.info(f"Financial accuracy test completed: {accuracy_percentage:.1f}% "
                   f"({successful_tests}/{total_tests} scenarios passed)")

    def test_explanation_quality_consistency(self):
        """Test explanation quality remains consistent across multiple generations."""
        analysis_data = {
            'symbol': 'ACCUR',
            'technical_score': 7.5,
            'recommendation': 'BUY',
            'analysis_date': datetime.now().isoformat(),
            'indicators': {
                'rsi': {'value': 58.2, 'signal': 'bullish', 'interpretation': 'Strong momentum'},
                'macd': {'value': 0.42, 'signal': 'bullish', 'interpretation': 'Bullish crossover'},
                'sma_50': {'value': 202.1, 'signal': 'bullish', 'interpretation': 'Price above SMA'}
            }
        }
        
        quality_metrics = []
        consistency_failures = []
        
        # Generate multiple explanations (5 iterations for consistency testing)
        for i in range(5):
            try:
                result = self.llm_service.generate_explanation(
                    analysis_data=analysis_data,
                    detail_level="standard"
                )
                
                if result and 'explanation' in result:
                    explanation = result['explanation']
                    
                    # Quality metrics
                    word_count = len(explanation.split())
                    contains_recommendation = 'BUY' in explanation.upper()
                    contains_indicators = any(indicator in explanation.lower() 
                                            for indicator in ['rsi', 'macd', 'sma', 'momentum'])
                    
                    quality_metrics.append({
                        'iteration': i + 1,
                        'word_count': word_count,
                        'contains_recommendation': contains_recommendation,
                        'contains_indicators': contains_indicators,
                        'explanation': explanation
                    })
                    
                    # Check for consistency issues
                    if not contains_recommendation:
                        consistency_failures.append(f"Iteration {i+1}: Missing BUY recommendation")
                    
                    if word_count < 50:
                        consistency_failures.append(f"Iteration {i+1}: Too short ({word_count} words)")
                    
                    if not contains_indicators:
                        consistency_failures.append(f"Iteration {i+1}: Missing indicator discussion")
                        
            except Exception as e:
                consistency_failures.append(f"Iteration {i+1}: Generation failed - {str(e)}")
        
        # Verify consistency requirements
        self.assertGreaterEqual(len(quality_metrics), 3,
                               "Should generate at least 3 successful explanations")
        
        self.assertEqual(len(consistency_failures), 0,
                        f"Quality consistency failures: {consistency_failures}")
        
        # Verify statistical consistency
        if quality_metrics:
            word_counts = [m['word_count'] for m in quality_metrics]
            avg_word_count = sum(word_counts) / len(word_counts)
            word_count_variance = sum((wc - avg_word_count) ** 2 for wc in word_counts) / len(word_counts)
            
            # Word count should not vary excessively (coefficient of variation < 0.5)
            if avg_word_count > 0:
                coefficient_of_variation = (word_count_variance ** 0.5) / avg_word_count
                self.assertLess(coefficient_of_variation, 0.5,
                               f"Excessive word count variation: {coefficient_of_variation:.2f}")
        
        logger.info(f"Quality consistency test: {len(quality_metrics)} explanations generated, "
                   f"avg length: {avg_word_count:.0f} words")